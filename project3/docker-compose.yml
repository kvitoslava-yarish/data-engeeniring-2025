version: "3.9"


x-airflow-env: &airflow_env
  AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW__CORE__LOAD_EXAMPLES}
  AIRFLOW__WEBSERVER__RBAC: ${AIRFLOW__WEBSERVER__RBAC}
  AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
  AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW_SECRET_KEY}
  AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "true"
  AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: "true"
  AIRFLOW__CORE__ENABLE_XCOM_PICKLING: "true"
  TZ: ${TZ}

networks:
  iceberg_network:
    driver: bridge

services:
  
  clickhouse:
    image: clickhouse/clickhouse-server:24.8
    container_name: clickhouse
    environment:
      TZ: ${TZ}
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: 1
      CLICKHOUSE_USER: ${CLICKHOUSE_USER}
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD}
      CLICKHOUSE_DB: youtube
      
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - ./clickhouse/init:/docker-entrypoint-initdb.d
    ports:
      - "8123:8123"
      - "9000:9000"
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8123/ping"]
      interval: 10s
      timeout: 5s
      retries: 10
    networks:
      iceberg_network:
    

  airflow-init:
    build:
      context: ./airflow
    container_name: airflow-init
    entrypoint: /bin/bash
    command: -c "chown -R ${AIRFLOW_UID}:${AIRFLOW_GID} /opt/airflow/logs && \
      chmod -R 777 /opt/airflow/logs && \
      airflow db migrate && \
      airflow users create --role Admin --username ${AIRFLOW_ADMIN_USER} --password ${AIRFLOW_ADMIN_PWD} --firstname Admin --lastname User --email ${AIRFLOW_ADMIN_EMAIL}"
    environment:
      _PIP_ADDITIONAL_REQUIREMENTS: "pyiceberg[s3fs,pyarrow] minio clickhouse-driver"
      <<: *airflow_env
      CLICKHOUSE_HOST: ${CLICKHOUSE_HOST}
      CLICKHOUSE_DB: ${CLICKHOUSE_DB}
      CLICKHOUSE_USER: ${CLICKHOUSE_USER}
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD}
      CLICKHOUSE_TCP_PORT: ${CLICKHOUSE_TCP_PORT}
    user: "0:0"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/airflow.db:/opt/airflow/airflow.db
      - ./airflow/include:/opt/airflow/include
      - ./dbt:/opt/dbt
    depends_on:
      clickhouse:
        condition: service_healthy
    networks:
      iceberg_network:

  airflow-webserver:
    build:
      context: ./airflow
    container_name: airflow-webserver
    user: "1000:0"
    restart: always
    environment:
      _PIP_ADDITIONAL_REQUIREMENTS: "pyiceberg[s3fs,pyarrow] minio clickhouse-driver"
      <<: *airflow_env
      CLICKHOUSE_HOST: ${CLICKHOUSE_HOST}
      CLICKHOUSE_DB: ${CLICKHOUSE_DB}
      CLICKHOUSE_USER: ${CLICKHOUSE_USER}
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD}
      CLICKHOUSE_TCP_PORT: ${CLICKHOUSE_TCP_PORT}
      DBT_TARGET_PATH: /tmp/dbt_target
    #    user: "${AIRFLOW_UID}:${AIRFLOW_GID}"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/airflow.db:/opt/airflow/airflow.db
      - ./airflow/include:/opt/airflow/include
      - ./dbt:/opt/dbt
      - ./airflow/data:/opt/airflow/data
      - ./airflow/scripts:/opt/airflow/scripts
    command: >
      bash -c "mkdir -p /opt/dbt/target &&
               chmod -R 777 /opt/dbt/target &&
               exec airflow webserver"
    ports:
      - "8083:8080"
    depends_on:
      airflow-scheduler:
        condition: service_started
    networks:
      iceberg_network:

  airflow-scheduler:
    build:
      context: ./airflow
    container_name: airflow-scheduler
    user: "1000:0"
    restart: always
    environment:
      _PIP_ADDITIONAL_REQUIREMENTS: "pyiceberg[s3fs,pyarrow] minio clickhouse-driver"
      <<: *airflow_env
      CLICKHOUSE_HOST: ${CLICKHOUSE_HOST}
      CLICKHOUSE_DB: ${CLICKHOUSE_DB}
      CLICKHOUSE_USER: ${CLICKHOUSE_USER}
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD}
      CLICKHOUSE_TCP_PORT: ${CLICKHOUSE_TCP_PORT}
    #    user: "${AIRFLOW_UID}:${AIRFLOW_GID}"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/airflow.db:/opt/airflow/airflow.db
      - ./airflow/include:/opt/airflow/include
      - ./dbt:/opt/dbt
      - ./airflow/data:/opt/airflow/data
      - ./airflow/scripts:/opt/airflow/scripts

    command: >
      bash -c "mkdir -p /opt/dbt/target &&
               chmod -R 777 /opt/dbt/target &&
               exec airflow scheduler"
    healthcheck:
      test: [ "CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname $(hostname)" ]
      interval: 10s
      timeout: 10s
      retries: 20
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      clickhouse:
        condition: service_healthy
    networks:
      iceberg_network:


  dbt-init:
    build:
      context: ./dbt
    container_name: dbt-init
    user: "1000:1000"
    working_dir: /usr/app
    entrypoint: /bin/bash
    command: -c "dbt deps && dbt build"
    environment:
      TZ: ${TZ}
      DBT_TARGET: ${DBT_TARGET}
      DBT_SCHEMA: ${DBT_SCHEMA}
      DBT_HOST: ${DBT_HOST}
      DBT_PORT: ${DBT_PORT}
      DBT_USER: ${DBT_USER}
      DBT_PASSWORD: ${DBT_PASSWORD}
      DBT_DATABASE: ${DBT_DATABASE}
      DBT_PROFILES_DIR: /usr/app/profiles
    volumes:
      - ./dbt:/usr/app
    depends_on:
      clickhouse:
        condition: service_healthy
    networks:
      iceberg_network:

  dbt:
    build:
      context: ./dbt
    container_name: dbt
    user: "1000:1000"
    working_dir: /usr/app
    environment:
      TZ: ${TZ}
      DBT_TARGET: ${DBT_TARGET}
      DBT_SCHEMA: ${DBT_SCHEMA}
      DBT_HOST: ${DBT_HOST}
      DBT_PORT: ${DBT_PORT}
      DBT_USER: ${DBT_USER}
      DBT_PASSWORD: ${DBT_PASSWORD}
      DBT_DATABASE: ${DBT_DATABASE}
      DBT_PROFILES_DIR: /usr/app/profiles
      DBT_TARGET_PATH: /tmp/dbt_target

    volumes:
      - ./dbt:/usr/app
    ports:
      - "8081:8081"
    command: tail -f /dev/null
    depends_on:
      dbt-init:
        condition: service_completed_successfully
    networks:
      iceberg_network:

  tabix:
    image: spoonest/clickhouse-tabix-web-client:latest
    container_name: tabix
    ports:
      - "8124:80"
    depends_on:
      clickhouse:
        condition: service_healthy

  minio:
    image: minio/minio:RELEASE.2024-10-13T13-34-11Z
    container_name: minio
    ports:
      - "9002:9000"   # MinIO API
      - "9003:9001"   # MinIO Console
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - ./data:/data
      - ./minio-init.sh:/usr/local/bin/minio-init.sh
    networks:
      iceberg_network:
        aliases:
          - minio
  
  duckdb_lab:
    build: .
    container_name: duckdb_lab
    volumes:
      - ./data:/lab/data
    depends_on:
      - minio
      - iceberg_rest
    stdin_open: true
    tty: true
    environment:
      PYICEBERG_HOME: /lab/data
      PYICEBERG_CATALOG__REST__URI: http://iceberg_rest:8181/
      PYICEBERG_CATALOG__REST__WAREHOUSE: s3://data-bucket/
      PYICEBERG_CATALOG__REST__IO__IMPL: org.apache.iceberg.aws.s3.S3FileIO
      PYICEBERG_CATALOG__REST__S3__ENDPOINT: http://minio:9000
      PYICEBERG_CATALOG__REST__S3__ACCESS-KEY-ID: minioadmin
      PYICEBERG_CATALOG__REST__S3__SECRET-ACCESS-KEY: minioadmin

    networks:
      - iceberg_network

  iceberg_rest:
    image: tabulario/iceberg-rest:1.6.0
    container_name: iceberg_rest
    ports:
      - "8181:8181"
    environment:
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      AWS_REGION: us-east-1
      AWS_ENDPOINT: http://minio:9000
    
      CATALOG_WAREHOUSE: s3://data-bucket/
      CATALOG_IO__IMPL: org.apache.iceberg.aws.s3.S3FileIO
      CATALOG_S3_ENDPOINT: http://minio:9000
      CATALOG_S3_PATH__STYLE__ACCESS: true

    depends_on:
      - minio
    networks:
      iceberg_network:

  superset:
    build:
      context: .
    container_name: superset
    environment:
      SUPERSET_SECRET_KEY: ${SUPERSET_SECRET_KEY}
      SUPERSET_LOAD_EXAMPLES: "no"
      SUPERSET_ADMIN_USERNAME: ${SUPERSET_ADMIN_USER}
      SUPERSET_ADMIN_PASSWORD: ${SUPERSET_ADMIN_PWD}
      SUPERSET_ADMIN_EMAIL: ${SUPERSET_ADMIN_EMAIL}
      SUPERSET_ADMIN_FIRST_NAME: Superset
      SUPERSET_ADMIN_LAST_NAME: Admin
    command: >
      bash -c "superset db upgrade &&
               superset fab create-admin --username ${SUPERSET_ADMIN_USER} --firstname Superset --lastname Admin --email ${SUPERSET_ADMIN_EMAIL} --password ${SUPERSET_ADMIN_PWD} || true &&
               superset init &&
               superset run -h 0.0.0.0 -p 8088"
    ports:
      - "8088:8088"
    depends_on:
      clickhouse:
        condition: service_healthy
    volumes:
      - superset_home:/app/superset_home
    networks:
      iceberg_network:

volumes:
  clickhouse_data:
  airflow_dags:
  airflow_logs:
  airflow_plugins:
  superset_home:
